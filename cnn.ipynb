{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNstEehUwm1hvMoQGjUOzGR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/statrav/cnn/blob/main/cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ],
      "metadata": {
        "id": "kN099hrN5cgV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이번 시간에는 MNIST 데이터셋을 통해 간단한 CNN 모델을 구현해보도록 하겠습니다.\n",
        "\n",
        "해당 프로젝트를 통해 CNN의 기본 작동 방식을 이해할 수 있을 것입니다. 프로젝트 시작에 앞서, MNIST 데이터셋과 CNN 모델에 대해 간단하게 알아보겠습니다.\n",
        "\n",
        "### 🎨 MNIST 데이터셋\n",
        "MNIST는 숫자 0부터 9까지의 손글씨 이미지를 가지고 있는 데이터셋입니다. 총 60,000개의 훈련 이미지와 10,000개의 테스트 이미지로 구성되어 있으며, 각 이미지는 28x28 픽셀 크기의 흑백 숫자 이미지입니다. MNIST 데이터셋은 손글씨 인식 문제를 해결하기 위한 대표적인 예제로 사용되며, 추가로 다양한 기계 학습 알고리즘의 성능을 평가하는 데 자주 활용되는 데이터셋입니다.\n",
        "\n",
        "### 🧱 CNN\n",
        "CNN(Convolutional Neural Network)은 주로 이미지 처리 분야에서 사용되는 딥러닝 모델입니다. 여러 개의 컨볼루션 레이어와 풀링 레이어를 사용하여 이미지의 특징을 추출하고, 이를 기반으로 분류 또는 예측 작업을 수행합니다. CNN은 특히 이미지 데이터에서 높은 성능을 발휘하며, 이미지 분류, 객체 탐지, 의료 영상 분석 등 다양한 응용 분야에서 널리 사용됩니다."
      ],
      "metadata": {
        "id": "-d7ZUHzC5ffA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 환경 셋팅\n",
        "\n",
        "모델 구현을 위해 필요한 모듈과 라이브러리를 적절하게 가져오겠습니다."
      ],
      "metadata": {
        "id": "II7Il-4y6jH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import time\n",
        "import os\n",
        "\n",
        "from tensorflow.python.keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D"
      ],
      "metadata": {
        "id": "zftzfBQJ6iOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터셋 로드 및 전처리\n",
        "\n",
        "가장 먼저 `keras.datasets.mnist.load_data()` 함수를 사용하여 MNIST 데이터셋을 로드합니다.\n",
        "\n",
        "그 다음, 이미지를 28x28 크기로 reshape하고, 픽셀 값을 0~1 범위로 정규화합니다.\n",
        "\n",
        "마지막으로 레이블을 one-hot 벡터로 변환합니다."
      ],
      "metadata": {
        "id": "NjlSZISr6ifu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_rows = 28\n",
        "img_cols = 28\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.  # 데이터 정규화\n",
        "x_test = x_test.astype('float32') / 255.  # 데이터 정규화\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "metadata": {
        "id": "5mZoPNVN6zJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 정의\n",
        "\n",
        "먼저 Sequential 모델을 생성합니다.\n",
        "\n",
        "그 다음, 두 개의 Conv2D 레이어와 두 개의 MaxPooling2D 레이어를 사용하여 특징을 추출합니다.\n",
        "\n",
        "이후 Dropout 레이어를 사용하여 과적합을 방지하겠습니다. 또한 Flatten 레이어를 사용하여 2D 특징 맵을 1차원 배열로 변환합니다.\n",
        "\n",
        "모든 변환이 완료되면, 두 개의 Dense 레이어를 사용하여 분류기를 구성합니다.\n",
        "첫 번째 Dense 레이어는 1000개의 뉴런을 가지며, 두 번째 Dense 레이어는 클래스 수(10개)만큼의 뉴런을 가지고 소프트맥스 활성화 함수를 사용합니다."
      ],
      "metadata": {
        "id": "By7h6fXT611a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 10\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)  # 학습을 위한 원핫벡터로 변경\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)  # 원핫벡터로 변경\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1), padding='same', activation='relu', input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Conv2D(64, (2, 2), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())  # fully connected layer\n",
        "model.add(Dense(1000, activation='relu'))  # 완전연결계층\n",
        "model.add(Dropout(0.5))  # 과대적합방지를 위해 사용\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Pt4F-djw5fHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 컴파일 및 학습\n",
        "\n",
        "이제 모델을 컴파일하고, 학습 데이터셋을 사용하여 모델을 학습시키도록 하겠습니다.\n",
        "\n",
        "학습 과정 중에는 정확도와 손실 값을 출력함으로써, 학습 과정을 확인할 것입니다.\n",
        "또한 학습 시간을 참고하기 위해, time 라이브러리를 통해 시간을 저장하겠습니다.\n",
        "\n",
        "마지막으로 테스트 데이터셋을 사용하여 모델의 최종 성능을 평가합니다."
      ],
      "metadata": {
        "id": "XxyiWjH_7xRK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQ3bc82-5OmT"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
        "\n",
        "end = time.time() - start"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test의 손실 값과 정확도를 확인하고, 총 학습 시간을 계산합니다.\n",
        "\n",
        "모든 과정이 종료되었습니다. 지금까지 CNN 모델을 실제 데이터셋을 활용하여 학습시켜보면서, CNN의 기본 작동 원리에 대해 알아보았습니다."
      ],
      "metadata": {
        "id": "ONOdRRMe7-4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "print('Training time:', end)"
      ],
      "metadata": {
        "id": "8ibqrmGn8GBp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}